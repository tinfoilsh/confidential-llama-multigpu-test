shim-version: v0.1.1@sha256:fcd0ba0ea3f0ca26d92595ea90f28308cfba819e3243925892e6c8f21eb1397d
cvm-version: 0.4.1
cpus: 32
memory: 524288
gpus: full
vllm: false

models:
  - name: "llama3-3-70b-fp8"
    repo: "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic@984c96b73bcf6a675945bac6382b9ed551e5d42b"
    mpk: "0d0be05062e4d3fadc17176cffd1ef6b518a02b0410bd897723f423074ec6cb5_72687374336_99f5f660-3ee0-5626-9772-2f082314e763"
  - name: "llama3-3-70b-int4"
    repo: "lambdalabs/Llama-3.3-70B-Instruct-AWQ-4bit@a70257cf10f368114a66115c315def76a1227e26"
    mpk: "06002d4871dbd882fc8b4c385e5181fdd85615e514f4efca3b6b3a3a774a36d2_39785426944_58b5d994-b471-58c9-8058-d224be115e1a"

containers:
  - name: "llama3-3-70b-fp8"
    image: ""
    args: [
      "--runtime", "nvidia",
      "--gpus", "'\"device=0,1\"'",
      "--ipc", "host",
      "vllm/vllm-openai:v0.9.2",
      "--model", "/tinfoil/mpk/mpk-0d0be05062e4d3fadc17176cffd1ef6b518a02b0410bd897723f423074ec6cb5",
      "--tensor-parallel-size", "2",
      "--max-model-len", "32768",
      "--served-model-name", "llama3-3-70b-fp8-tp2",
      "--port", "8001"
    ]
  - name: "llama3-3-70b-int4"
    image: ""
    args: [
      "--runtime", "nvidia",
      "--gpus", "'\"device=2\"'",
      "--ipc", "host",
      "vllm/vllm-openai:v0.9.2",
      "--model", "/tinfoil/mpk/mpk-06002d4871dbd882fc8b4c385e5181fdd85615e514f4efca3b6b3a3a774a36d2",
      "--tensor-parallel-size", "1",
      "--max-model-len", "32768",
      "--served-model-name", "llama3-3-70b-int4",
      "--port", "8002",
      "--quantization", "awq_marlin"
    ]
  - name: "model-router"
    image: ""
    args: [
      "ghcr.io/tinfoilsh/model-router:0.0.1",
      "/app/bin", "-m", "llama3-3-70b-fp8-tp2_8001,llama3-3-70b-int4_8002",
    ]

shim:
  listen-port: 443
  upstream-port: 8087
  publish-attestation: false
  tls-challenge: dns
  tls-mode: self-signed  
  paths:
    - /v1/chat/completions
    - /metrics
  origins:
    - https://tinfoil.sh
    - https://chat.tinfoil.sh
    - http://localhost:3000
